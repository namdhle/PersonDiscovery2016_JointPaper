\section{Experiments}
\label{sec:experiment}

Contrastive experiments with various configurations are conducted for each approach to better understanding them. Then we compare all approaches for further analysis. All figures are reported using Person Discovery benchmark dataset and the metrics is MAP@K $(K \in {1, 10, 100})$. MAP@10 is used as the primary number for comparison.

\subsection{Contrastive Results}

\mypartitle{Clustering-based naming.} Tab.~\ref{tab:clustering} shows the results using CBM with different settings. First, we test a system based solely on speaker diarization (A), which is common for both LIMSI and EUMSSI. The precision is greatly far behind the baseline. This is because speech turns are wrongly over-clustered due to dubbing and voice-over.
%
When comparing 2 face clustering methods, LIMSI (V) outperforms EUMSSI (V) at MAP@1 while being slightly behind in MAP@10. This can be explained by the more robust detector used in EUMSSI (V) which detects faces at multiple poses while LIMSI (V) only detects frontal faces which has higher precision.
%
This also explains why after applying talking face detection, EUMSSI (V-talking) has a significant increase while LIMSI (V-talking) only has a minor improvement. People appearing in frontal faces often are those who talks as well.
%
Finally when AV results are fused, we can observe a substantial improvement in both systems.

\begin{table}[tb]
\centering
\caption{Benchmarking results of clustering-based naming systems.}
\vspace*{-2mm}
\begin{tabular}{c|c|c|c|| c|c|c|}
\cline{2-7}
  &  \multicolumn{3}{|c||}{LIMSI} &  \multicolumn{3}{|c|}{EUMSSI} \\ \cline{2-7}
           & @1& @10& @100   & @1& @10& @100 \\ \hline
 \multicolumn{1}{|c|}{A} & 29.9   & 26.2   & 25.2  & 29.9   & 26.2   & 25.2\\ \hline
 \multicolumn{1}{|c|}{V} & 65.8   & 46.0   & 45.0 & 62.3   & 50.3   & 49.2 \\ \hline
 \multicolumn{1}{|c|}{V-Talking} & 66.3   & 46.3   & 45.4 & 69.3   & 57.0   & 55.8 \\ \hline
 \multicolumn{1}{|c|}{AV} & 67.8   & 47.4   & 46.4 & 73.6   & 59.8   & 57.9\\ \hline
\end{tabular}
%
\vspace*{-5mm}
\label{tab:clustering}
\end{table}


\mypartitle{Verification-based naming.} Tab.~\ref{tab:verification} shows the results achieved with UVigo and UPC systems. UVigo systems perform better on audio domain than on visual one because the face system only verifies the most dominant face of each shot.
%
Meanwhile for UPC systems, the one based on face verification works better than that of speech processing. UPC face system also has problem when multiple individuals are associated with a single text name. Similarly to CBN, speech processing system cannot be used individually to perform in this task. It must be combined with other face system.
%
Multimodal systems slightly improved the performance of monomodal approaches.

\begin{table}[tb]
\centering
\caption{GTM-UVigo and UPC results using audio, video and multimodal approaches.}
\vspace*{-2mm}
\begin{tabular}{c|c|c|c|| c|c|c|}
\cline{2-7}
  &  \multicolumn{3}{|c||}{UVigo} &  \multicolumn{3}{|c|}{UPC} \\ \cline{2-7}
           & @1& @10& @100   & @1& @10& @100 \\ \hline
 \multicolumn{1}{|c|}{A} &  44.1  & 36.9   & 35.9  &  40.1  & 35.1   & 34.7\\ \hline
 \multicolumn{1}{|c|}{V} &  40.9  & 37.1   & 35.7 &  56.7  & 42.5   & 41.9 \\ \hline
 \multicolumn{1}{|c|}{AV} & 45.6 & 38.4 & 37.0 & 54.8 & 45.8 & 45.1 \\ \hline
\end{tabular}
%
\vspace*{-5mm}
\label{tab:verification}
\end{table}

\mypartitle{Graph-based naming.} Tab.~\ref{tab:graph} gathers the performances obtained by the graph-based systems. In all cases, graph propagation enhances the results over the baseline by around 20\%. The audiovisual version of RW improves the performances for RW by over 2\% w.r.t. visual version only, which is not the case for MST (over 1\%). This case is in favor of a better tuning of the fusion parameters.
%
Overall, the hierarchical tag propagation on graphs  combining  CNN  visual  similarity  and  binary  voice similarity (primary) consistently outperforms other combinations, showing the interest of combining audio and visual similarities.  


\begin{table}[tb]
\label{tab:graph}
\centering
\vspace*{-5mm}
\caption{Mean Average Precisions @K obtained by graph-based systems.}
\vspace*{-2mm}
\begin{tabular}{c|c|c|c|| c|c|c|}
\cline{2-7}
  &  \multicolumn{3}{|c||}{RW} &  \multicolumn{3}{|c|}{MST} \\ \cline{2-7}
           & @1& @10& @100   & @1& @10& @100 \\ \hline
 \multicolumn{1}{|c|}{A} & 67.3 &  51.6 & 50.1  & 62.9 &  50.1 & 48.6\\ \hline
 \multicolumn{1}{|c|}{V} & 69.3  & 53.8 & 52.1 & 70.5  & 56.0 & 54.3\\ \hline
 \multicolumn{1}{|c|}{AV} & 71.3 &  57.4 & 55.5 &  68.9 &  55.4 & 53.6\\ \hline
\end{tabular}
\vspace*{-5mm}
\end{table}

\subsection{Analysis of 3 approaches}

Out of 3 strategies, VBN achieves inferior results. This may due to the fact that the verification models are trained using only 1 track, which does not contains enough variation. 
%
Moreover, this approach relies on the quality of OCR-NER. A false name can leads to spreading of wrong names to multiple shots. In the future, some early clustering can help to increase the size of training data while some text filtering can increase the precision of enrollment.
%
On the other hand, GBN requires a face track and a speech turn to be sufficiently overlapped before assigning a name, thus reducing the effect of false texts. Also the combination of AV similarities force a face and a voice to have the same identity to be tagged, which implicitly performs talking face detection.
%
However, discriminative talking detection model still outperforms when applied in CBN systems. Therefore, using this taking face detector in GBN is an interesting future work. 
%
VBN can be used to learn more discriminative similarity for GBN edges.
%
Lastly, the effectiveness of combining with audio and visual results is still not as significant as other improvements. This requires further experiments in the future to fully exploit the potential of multimodal processing.

\endinput

\subsubsection{Face}

\begin{table}[tb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\cline{3-5}
 \multicolumn{2}{c|}{ }	& MAP@1  & MAP@10 & MAP@100  \\ \hline
 \multirow{2}{*}{Clus.} & S1	& 66.3   & 46.3   & 45.4 \\ \cline{2-5}
 						& S2  	& 69.3   & 57.0   & 55.8 \\ \hline

 \multirow{2}{*}{Verf.} & S1	& 40.9   & 37.1   & 35.7 \\ \cline{2-5}
 						& S2 	& 00.0   & 00.0   & 00.0 \\ \hline

 \multirow{2}{*}{Grap.} & RW 	& 69.3   & 53.8   & 52.1 \\ \cline{2-5}
 						& MST 	& 70.5   & 56.0   & 54.3 \\ \hline 										
\end{tabular}
\vspace*{-2mm}
\caption{Results of systems using only visual modality.}
\vspace*{-2mm}
\label{tab:face_result}
\end{table}

\subsubsection{Speech}

\begin{table}[tb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\cline{3-5}
 \multicolumn{2}{c|}{ }	& MAP@1 & MAP@10 & MAP@100  \\ \hline
 \multirow{1}{*}{Clus.} & LIUM	& 29.9   & 26.2   & 25.2 \\ \cline{1-5}

 \multirow{2}{*}{Verf.} & S1	& 44.1   & 36.9   & 35.9 \\ \cline{2-5}
 						& S2 	& 00.0   & 00.0   & 00.0 \\ \hline

 \multirow{2}{*}{Grap.} & RW 	& 67.3 	 &  51.6  & 50.1 \\ \cline{2-5}
 						& MST 	& 62.9   &  50.1  & 48.6 \\ \hline 								
\end{tabular}
\vspace*{-2mm}
\caption{Results of systems using only speech modality.}
\vspace*{-2mm}
\label{tab:speech_result}
\end{table}

\subsubsection{Audiovisual}

\begin{table}[tb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\cline{3-5}
 \multicolumn{2}{c|}{ }	& MAP@1 & MAP@10 & MAP@100  \\ \hline
 \multirow{1}{*}{Clus.} & S1	& 67.8   & 47.4   & 46.4 \\ \cline{2-5}
 						& S2 	& 73.6   & 59.8   & 57.9 \\ \hline
 						
 \multirow{1}{*}{Verf.} & S1	& 45.6   & 38.4   & 37.0\\ \cline{2-5}
 						& S2 	& 00.0   & 00.0   & 00.0 \\ \hline

 \multirow{2}{*}{Grap.} & RW 	& 71.3   &  57.4  & 55.5 \\ \cline{2-5}
 						& MST 	& 68.9   &  55.4  & 53.6 \\ \hline 								
\end{tabular}
\vspace*{-2mm}
\caption{Results of audiovisual systems systems.}
\vspace*{-2mm}
\label{tab:av_result}
\end{table}

\subsubsection{Discussion}

\endinput
